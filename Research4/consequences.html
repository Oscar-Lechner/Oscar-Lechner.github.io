<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>CONSEQUENCES</title>
    <link rel="stylesheet" href="styles.css">
</head>

<body>
    <h1> AI IN THE COURTROOM</h1> 

        <nav>
            <ul>
                <br>
                <li><a href="https://alyssacady.me/Research4/homepage4.html">HOME</a></li> 
                <li><a href="https://alyssacady.me/Research4/background.html">BACKGROUND</a></li>
                <li><a href="consequences.html">CONSEQUENCES</a></li>
                <li><a href="solutions.html">SOLUTIONS</a></li>
            </ul>
        </nav> 

        <h2>UNINTENDED CONSEQUENCES</h2>
        <p>Despite their objective and artificial nature, AI judges can still be biased. <br>
            An AI model is only as good as the data it is trained on.
        </p>
        
        

        <article>
            <div id ="no1">
                <h3>Racial and gender bias:</h3>
                <h4> 
                    - AI-driven assessment tools, such as COMPAS, can inadvertently perpetuate racial and socioeconomic biases. 
                    For example, a report by the National Institute of Standards and Technology found that facial recognition algorithms 
                    used by police created more false positive results when evaluating images of Black women. <br>
                    - There have been concerns about their accuracy and fairness. For example, a study by ProPublica found that the 
                    COMPAS tool falsely flagged Black defendants as future criminals at almost twice the rate of white defendants, 
                    highlighting potential racial bias in the algorithm.
                </h4>
            </div> 

            <div id ="no2">
                <h3>Loss of Empathy</h3>
                <h4>
                    - AI judges lack the ability to understand and empathize with the human condition, which is a crucial element in the justice system. <br>
                    - This can lead to decisions that are technically correct but fail to consider the unique circumstances of individual cases.



                </h4>
                
            </div> 

            <div id ="no3">
                <h3>Training:</h3>
                <h4>  
                    - AI can perpetuate discrimination if allowed <br>
                    - Predictive policing applications, such as PredPol, may be tainted by being trained on "dirty data." 
                    This refers to historical data that includes biases or errors, such as arrest records and police reports that have 
                    not been cleaned or corrected for documented violations. As a result, AI systems trained on this data may lead to 
                    excessive or insufficient deployment of police resources to the same communities that were the subject of the tainted data.
                    <img src ="aijudge.jpg" src ="court room"> 
               </h4>
            </div> 
        </article>

        <h5>OTHER UNINTENDED CONSEQUENCES:</h5>
<p>
    - Over-reliance on AI: There's a risk that the legal system might become overly dependent on AI, leading to a reduction in human oversight. <br>
    - Transparency and Accountability: AI algorithms can be complex and opaque, making it difficult to understand how decisions are made. <br>
    - Privacy Concerns: AI policing tools often rely on surveillance and data collection, raising concerns about privacy. <br>
    - Legal and Ethical Challenges: The use of AI in the justice system raises numerous legal and ethical questions. <br>
    - Economic Impacts: The implementation of AI in the justice system could lead to job displacement for legal professionals. <br>
    - Technological Limitations: AI systems are not infallible and can suffer from technical limitations. <br>
</p>



        
           
        
</body>
</html>